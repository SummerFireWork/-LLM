{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "os.environ['http_proxy'] = os.getenv(\"http_proxy\")\n",
    "os.environ['https_proxy'] = os.getenv(\"https_proxy\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = os.getenv(\"DASHSCOPE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. StructuredOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 案例一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 告诉他我们生成的内容需要哪些字段，每个字段类型式啥\n",
    "response_schemas = [\n",
    "    ResponseSchema(type=\"string\", name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(type=\"string\", name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# 初始化解析器\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# 生成的格式提示符\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "print(\"=\"*200)\n",
    "\n",
    "# 加入至template中\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# 将我们的格式描述嵌入到prompt中去，告诉llm我们需要他输出什么样格式的内容\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# 假设一个提问，看看prompt是怎样的\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```json\\n{\\n\\t\"bad_string\": \"well come to the califonya\",\\n\\t\"good_string\": \"Welcome to California\"\\n}\\n```'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "# 进行实际问答\n",
    "llm = Tongyi(model = 'qwen-turbo')\n",
    "chain = (\n",
    "    prompt\n",
    "    | llm\n",
    ")\n",
    "chain.invoke('well come to the califonya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 案例二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"number\": number  // 文本中的数字\n",
      "\t\"people\": string  // 文本中的人物\n",
      "\t\"place\": string  // 文本中的地点\n",
      "}\n",
      "```\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "给定下面的文本，找出特定的结构化信息。\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"number\": number  // 文本中的数字\n",
      "\t\"people\": string  // 文本中的人物\n",
      "\t\"place\": string  // 文本中的地点\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "张晓明今天在香港坐了2趟地铁。\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "\n",
    "llm = Tongyi(model = 'qwen-turbo')\n",
    "\n",
    "# 告诉他我们生成的内容需要哪些字段，每个字段类型式啥\n",
    "response_schemas = [\n",
    "    ResponseSchema(type=\"number\", name=\"number\", description=\"文本中的数字\"),\n",
    "    ResponseSchema(type=\"string\", name=\"people\", description=\"文本中的人物\"),\n",
    "    ResponseSchema(type=\"string\", name=\"place\", description=\"文本中的地点\"),\n",
    "]\n",
    "\n",
    "# 初始化解析器\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# 生成的格式提示符\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "print(\"=\"*200)\n",
    "\n",
    "template = \"\"\"\n",
    "给定下面的文本，找出特定的结构化信息。\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "question = \"张晓明今天在香港坐了2趟地铁。\"\n",
    "## 看看提示词是怎么样的\n",
    "promptValue = prompt.format(user_input=\"张晓明今天在香港坐了2趟地铁。\")\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 第一种链条输出方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"number\": 2,\n",
      "\t\"people\": \"张晓明\",\n",
      "\t\"place\": \"香港\"\n",
      "}\n",
      "```\n",
      "========================================================================================================================================================================================================\n",
      "{'number': 2, 'people': '张晓明', 'place': '香港'}\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    prompt | llm\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"user_input\": question}))\n",
    "print(\"=\"*200)\n",
    "# 使用解析器进行解析生成的内容\n",
    "print(output_parser.parse(chain.invoke({\"user_input\": question})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 第二种链条输出方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number': 2, 'people': '张晓明', 'place': '香港'}\n"
     ]
    }
   ],
   "source": [
    "# 在链条中使用解析器进行解析生成的内容\n",
    "chain2 = (\n",
    "    prompt | llm | output_parser\n",
    ")\n",
    "\n",
    "print(chain2.invoke({\"user_input\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 案例三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"time\": array  // 文本中的日期时间列表\n",
      "\t\"people\": array  // 文本中的人物列表\n",
      "\t\"place\": array  // 文本中的地点列表\n",
      "\t\"org\": array  // 文本中的组织机构列表\n",
      "}\n",
      "```\n",
      "========================================================================================================================================================================================================\n",
      "\n",
      "给定下面的文本，找出特定的实体信息，并以结构化数据格式返回。\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"time\": array  // 文本中的日期时间列表\n",
      "\t\"people\": array  // 文本中的人物列表\n",
      "\t\"place\": array  // 文本中的地点列表\n",
      "\t\"org\": array  // 文本中的组织机构列表\n",
      "}\n",
      "```\n",
      "\n",
      "% USER INPUT:\n",
      "6月26日，广汽集团在科技日上首次公开展示飞行汽车项目，飞行汽车GOVE完成全球首飞。广汽研究院院长吴坚表示，GOVE可以垂直起降，并搭载双备份多旋翼飞行系统，保障飞行安全。\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n",
      "prompt: ========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import Tongyi\n",
    "\n",
    "llm = Tongyi(model = 'qwen-turbo')\n",
    "\n",
    "# 告诉他我们生成的内容需要哪些字段，每个字段类型式啥\n",
    "response_schemas = [\n",
    "    ResponseSchema(type=\"array\", name=\"time\", description=\"文本中的日期时间列表\"),\n",
    "    ResponseSchema(type=\"array\", name=\"people\", description=\"文本中的人物列表\"),\n",
    "    ResponseSchema(type=\"array\", name=\"place\", description=\"文本中的地点列表\"),\n",
    "    ResponseSchema(type=\"array\", name=\"org\", description=\"文本中的组织机构列表\"),\n",
    "]\n",
    "\n",
    "# 初始化解析器\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# 生成的格式提示符\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "print(\"=\"*200)\n",
    "\n",
    "template = \"\"\"\n",
    "给定下面的文本，找出特定的实体信息，并以结构化数据格式返回。\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "% USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "question = \"6月26日，广汽集团在科技日上首次公开展示飞行汽车项目，飞行汽车GOVE完成全球首飞。广汽研究院院长吴坚表示，GOVE可以垂直起降，并搭载双备份多旋翼飞行系统，保障飞行安全。\"\n",
    "# prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    "    template=template\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_input = question)\n",
    "print(promptValue)\n",
    "print(\"prompt:\", \"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': ['6月26日'], 'people': ['吴坚'], 'place': [], 'org': ['广汽集团', '广汽研究院']}\n"
     ]
    }
   ],
   "source": [
    "# 在链条中使用解析器进行解析生成的内容\n",
    "chain = (\n",
    "    prompt | llm | output_parser\n",
    ")\n",
    "\n",
    "print(chain.invoke({\"user_input\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 案例四"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe': '1. Heat olive oil in a large pot over medium heat. Add chopped onions, carrots, and celery. Cook until softened.\\n2. Add ground beef and cook until browned, stirring to break up the meat.\\n3. Stir in garlic, tomato paste, canned tomatoes, red wine (optional), and herbs. Simmer for at least 1 hour, stirring occasionally.\\n4. Cook spaghetti according to package instructions until al dente. Drain.\\n5. Serve the sauce over the spaghetti and top with grated Parmesan cheese if desired.',\n",
       " 'ingredients': '- 1 pound ground beef\\n- 1 large onion, finely chopped\\n- 2 carrots, finely chopped\\n- 2 stalks celery, finely chopped\\n- 3 cloves garlic, minced\\n- 2 tablespoons olive oil\\n- 2 tablespoons tomato paste\\n- 1 can (28 ounces) crushed tomatoes\\n- 1 cup red wine (optional)\\n- 1 teaspoon dried oregano\\n- 1 teaspoon dried basil\\n- Salt and pepper to taste\\n- 1 pound spaghetti\\n- Grated Parmesan cheese (for serving)'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "# Define the schema for the expected output, including two fields: \"recipe\" and \"ingredients\"\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"recipe\", description=\"the recipe for the dish requested by the user\"),\n",
    "    ResponseSchema(\n",
    "        name=\"ingredients\",\n",
    "        description=\"list of ingredients required for the recipe, should be a detailed list.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create a StructuredOutputParser instance from the defined response schemas\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Generate format instructions based on the response schemas, which will be injected into the prompt\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Define the prompt template, instructing the model to provide the recipe and ingredients\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Provide the recipe for the dish requested.\\n{format_instructions}\\n{dish}\",\n",
    "    input_variables=[\"dish\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "model = ChatTongyi(temperature=0)\n",
    "\n",
    "# Create a chain that connects the prompt, model, and output parser\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# The output will be structured according to the predefined schema with fields for \"recipe\" and \"ingredients\"\n",
    "chain.invoke({\"dish\": \"Spaghetti Bolognese\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. With_structured_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class method takes an input schema to guide the LLM to generate specific responses.\n",
    "\n",
    "You can only use this with LLMs that provide APIs for structuring outputs, such as tool calling or JSON mode (this means it only works for providers like OpenAI, Anthropic, Cohere, etc.).\n",
    "\n",
    "If the model doesn’t natively support such features, you’ll need to use an output parser to extract the structured response from the model output.\n",
    "\n",
    "You typically use with_structured_output to specify a particular format you want the LLM to use in its response, passing this format as schema into the prompt. ‍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of with_structured_output**\n",
    "- Type safety: The Pydantic model ensures that the output matches the expected types.\n",
    "- Integration with LangChain’s runnables: This method wraps the LLM call in a runnable, allowing for easy chaining of operations.\n",
    "- Flexibility: You can use various schema types, including TypeDict, JSON schema, or Pydantic classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations and Considerations**\n",
    "- While powerful, with_structured_output has some limitations to keep in mind:\n",
    "- LLM compatibility: It only works with LLMs that support structured output APIs.\n",
    "- Complexity in advanced scenarios: When dealing with more complex workflows, the use of runnables can introduce additional complexity.\n",
    "- Learning curve: Understanding LangChain’s expression language (LCEL) and runnables requires some additional learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: input_variables=['topic'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Give me a trivia question about {topic}, respond in JSON with `question` and `answer` keys'), additional_kwargs={})]\n",
      "========================================================================================================================================================================================================\n",
      "Result: question='Who is the lead singer of the band Coldplay?' answer='Chris Martin'\n",
      "========================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model = ChatTongyi(temperature=0)\n",
    "\n",
    "class Trivia(BaseModel):\n",
    "    question: str = Field(description=\"The trivia question\")\n",
    "    answer: str = Field(description=\"The correct answer to the trivia question\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me a trivia question about {topic}, respond in JSON with `question` and `answer` keys\"\n",
    ")\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"=\"*200)\n",
    "\n",
    "# Create a structured LLM using the `with_structured_output` method\n",
    "structured_llm = model.with_structured_output(Trivia)\n",
    "\n",
    "# Chain the prompt and structured LLM using the pipe operator\n",
    "trivia_chain = prompt | structured_llm\n",
    "\n",
    "# Invoke the chain\n",
    "result = trivia_chain.invoke({\"topic\": \"music\"})\n",
    "\n",
    "print(\"Result:\", result)\n",
    "print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature=23.5 condition='sunny with a chance of rain' humidity=45 wind_speed=15.0\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.pydantic_v1 import BaseModel, Field \n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "from langchain_core. prompts import ChatPromptTemplate\n",
    "# Define our WeatherForecast model\n",
    "class WeatherForecast(BaseModel): \n",
    "    temperature: float = Field(description=\"The temperature in Celsius\")\n",
    "    condition: str = Field(description=\"The weather condition (e.g., sunny, rainy, cloudy)\")\n",
    "    humidity: int = Field(description=\"The humidity percentage\")\n",
    "    wind_speed: float = Field(description=\"The wind speed in km/h\")\n",
    "\n",
    "# Set up the LLM\n",
    "model = ChatTongyi(temperature=0)\n",
    "\n",
    "# Create the prompt template \n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"Given the context below, provide a weather forecast for {city} tomorrow, respond in JSON with temperature`, `condition`, `humidity`, and `wind_speed` keys\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "# Apply structured output to the LLM\n",
    "structured_llm = model.with_structured_output(WeatherForecast)\n",
    "\n",
    "# Chain the prompt and structured LLM\n",
    "weather_chain = prompt | structured_llm\n",
    "\n",
    "# Generate a weather forecast\n",
    "result = weather_chain.invoke({\"city\": \"New York\", \"context\": \"The weather in New York will be sunny with a chance of rain.\"})\n",
    "\n",
    "print(result)\n",
    "# Output: WeatherForecast(temperature=22.5, condition=' Partly cloudy', humidity=65, wind_speed=10.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PydanticOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all models support tool calling and JSON mode, so another approach for getting structured outputs is to use an output parser like PydanticOutputParser to extract the needed information.\n",
    "\n",
    "This parser works best in situations where type safety is an obvious concern, as it ensures that LLM responses adhere strictly to a Pydantic model schema.\n",
    "\n",
    "This parser also implements the LangChain runnable interface.\n",
    "\n",
    "Below, we use PydanticOutputParser to specify a Book schema to ensure the title in the LLM’s output is a string and the number of pages an integer: ‍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PydanticOutputParser class is another tool in LangChain's arsenal for extracting structured information from LLM outputs.\n",
    "\n",
    "It’s particularly useful when working with LLMs that don’t support native structured output features.\n",
    "\n",
    "It allows you to enforce type safety by ensuring that the output conforms to a predefined Pydantic schema.\n",
    "\n",
    "This parser works best when you need reliable type validation or when your LLM lacks structured output features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benefits of PydanticOutputParser**\n",
    "- Strict type enforcement: Ensures that the LLM output matches the expected data types.\n",
    "- Flexible schema definition: Allows for complex nested structures and custom validation logic.\n",
    "- Integration with LangChain’s runnable interface: Enables easy incorporation into LangChain workflows.\n",
    "- Limitations and Considerations\n",
    "- While powerful, PydanticOutputParser has some limitations to keep in mind:\n",
    "\n",
    "**Overhead: Requires additional processing to parse the LLM output.**\n",
    "- Prompt engineering: Needs careful prompt design to guide the LLM in producing the correct output format.\n",
    "- Learning curve: Requires familiarity with Pydantic for defining schemas.\n",
    "- The StructuredOutputParser Class\n",
    "- The StructuredOutputParser class is a versatile tool in LangChain that allows you to extract structured information from LLM outputs using custom-defined schemas.\n",
    "\n",
    "It works with diverse models and provides flexibility to define custom output structures through ResponseSchema objects.\n",
    "\n",
    "This parser is particularly useful when you need to extract information that doesn’t fit neatly into pre-built schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['query'] input_types={} partial_variables={'format_instructions': 'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"$defs\": {\"Movie\": {\"properties\": {\"title\": {\"description\": \"The title of the movie\", \"title\": \"Title\", \"type\": \"string\"}, \"director\": {\"description\": \"The director of the movie\", \"title\": \"Director\", \"type\": \"string\"}, \"runtime\": {\"description\": \"The runtime of the movie in minutes\", \"title\": \"Runtime\", \"type\": \"integer\"}}, \"required\": [\"title\", \"director\", \"runtime\"], \"title\": \"Movie\", \"type\": \"object\"}}, \"properties\": {\"movies\": {\"items\": {\"$ref\": \"#/$defs/Movie\"}, \"title\": \"Movies\", \"type\": \"array\"}}, \"required\": [\"movies\"]}\\n```'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions'], input_types={}, partial_variables={}, template='Answer the user query about movies in the film festival. Wrap the output in `json` format following the schema below\\n {format_instructions}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='{query}'), additional_kwargs={})]\n",
      "========================================================================================================================================================================================================\n",
      "movies=[Movie(title='Inception', director='Christopher Nolan', runtime=148), Movie(title='Parasite', director='Bong Joon-ho', runtime=132)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List \n",
    "from langchain_core.output_parsers import PydanticOutputParser \n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field  # LangChain *200官方案例是这样的\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "# Define our Movie and FilmFestival models \n",
    "\n",
    "class Movie(BaseModel): \n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    runtime: int = Field(..., description=\"The runtime of the movie in minutes\")\n",
    "\n",
    "class FilmFestival(BaseModel): \n",
    "    movies: List[ Movie]\n",
    "    \n",
    "# Set up the parser \n",
    "parser = PydanticOutputParser(pydantic_object=FilmFestival)\n",
    "\n",
    "# Create the prompt template \n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user query about movies in the film festival. Wrap the output in `json` format following the schema below\\n {format_instructions}\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "print(prompt)\n",
    "print(\"=\"*200)\n",
    "\n",
    "# Set up the LLM and chain \n",
    "llm = ChatTongyi(temperature=0)\n",
    "chain = prompt | llm | parser\n",
    "# Generate movie information \n",
    "query = \"Please provide details about the movies `Inception` directed by Christopher Nolanlwith a runtime of 148 minutes and `Parasite` directed by Bong Joon-ho with a runtime of 132 minutes.\"\n",
    "result = chain.invoke({\"query\": query})\n",
    "print(result)\n",
    "# Expected output: FilmFestival(movies=[ Movie(title=' Inception', director=' Christopher Nolan', runtime=148), Movie(title=' Parasite', director=' Bong Joon-ho', runtime=132)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. Wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Book\": {\"description\": \"Information about a book.\", \"properties\": {\"title\": {\"description\": \"The title of the book\", \"title\": \"Title\", \"type\": \"string\"}, \"pages\": {\"description\": \"The number of pages in the book.\", \"title\": \"Pages\", \"type\": \"integer\"}}, \"required\": [\"title\", \"pages\"], \"title\": \"Book\", \"type\": \"object\"}}, \"description\": \"Details about all books in a collection.\", \"properties\": {\"books\": {\"items\": {\"$ref\": \"#/$defs/Book\"}, \"title\": \"Books\", \"type\": \"array\"}}, \"required\": [\"books\"]}\n",
      "```\n",
      "Human: Please provide details about the books 'The Great Gatsby' with 208 pages and 'To Kill a Mockingbird' with 384 pages.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Any\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# from langchain_community.output_parsers.pydantic import PydanticOutputParser\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"Information about a book.\"\"\"\n",
    "\n",
    "    title: str = Field(..., description=\"The title of the book\")\n",
    "    pages: int = Field(\n",
    "        ..., description=\"The number of pages in the book.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Library(BaseModel):\n",
    "    \"\"\"Details about all books in a collection.\"\"\"\n",
    "\n",
    "    books: List[Book]\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Library)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# Query\n",
    "query = \"Please provide details about the books 'The Great Gatsby' with 208 pages and 'To Kill a Mockingbird' with 384 pages.\"\n",
    "\n",
    "# Print the prompt and output schema\n",
    "print(prompt.invoke(query).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books=[Book(title='The Great Gatsby', pages=208), Book(title='To Kill a Mockingbird', pages=384)]\n"
     ]
    }
   ],
   "source": [
    "llm = ChatTongyi(temperature=0)\n",
    "chain = prompt | llm | parser\n",
    "# Generate movie information \n",
    "result = chain.invoke({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies=[Movie(title='盗梦空间', director='克里斯托弗·诺兰', runtime=148), Movie(title='寄生虫', director='奉俊昊', runtime=132)]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field  # 确保是从 pydantic 导入而不是 langchain_core.pydantic_v1\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "# 定义我们的 Movie 和 FilmFestival 模型\n",
    "class Movie(BaseModel):\n",
    "    title: str = Field(..., description=\"电影的标题\")\n",
    "    director: str = Field(..., description=\"电影的导演\")\n",
    "    runtime: int = Field(..., description=\"电影的时长（分钟）\")\n",
    "\n",
    "class FilmFestival(BaseModel):\n",
    "    movies: List[Movie]\n",
    "\n",
    "# 设置解析器\n",
    "parser = PydanticOutputParser(pydantic_object=FilmFestival)\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"回答用户关于电影节中电影的问题。按照下面的模式将输出包裹在 `json` 格式中\\n {format_instructions}\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "]).partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "# 设置 LLM 和链\n",
    "llm = ChatTongyi(temperature=0)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "# 生成电影信息\n",
    "query = \"请提供由克里斯托弗·诺兰执导、片长148分钟的《盗梦空间》和由奉俊昊执导、片长132分钟的《寄生虫》的详细信息。\"\n",
    "result = chain.invoke({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
